{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 03.05"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Perceptron code on the website is a batch update algorithm, where the whole of the dataset is fed in to find the errors, and then the weights are updated afterwards, as is discussed in Section 3.3.5. Convert the code to run as sequential updates and then compare the results of using the two versions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first import `numpy`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the original code as found on the website:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pcn_batch:\n",
    "    \"\"\" A basic Perceptron\"\"\"\n",
    "    \n",
    "    def __init__(self,inputs,targets):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        # Set up network size\n",
    "        if np.ndim(inputs)>1:\n",
    "            self.nIn = np.shape(inputs)[1]\n",
    "        else: \n",
    "            self.nIn = 1\n",
    "    \n",
    "        if np.ndim(targets)>1:\n",
    "            self.nOut = np.shape(targets)[1]\n",
    "        else:\n",
    "            self.nOut = 1\n",
    "\n",
    "        self.nData = np.shape(inputs)[0]\n",
    "    \n",
    "        # Initialise network\n",
    "        self.weights = np.random.rand(self.nIn+1,self.nOut)*0.1-0.05\n",
    "\n",
    "    def pcntrain(self,inputs,targets,eta,nIterations):\n",
    "        \"\"\" Train the thing \"\"\" \n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "        # Training\n",
    "        change = range(self.nData)\n",
    "\n",
    "        for n in range(nIterations):\n",
    "            \n",
    "            self.activations = self.pcnfwd(inputs);\n",
    "            self.weights -= eta*np.dot(np.transpose(inputs),self.activations-targets)\n",
    "        \n",
    "            # Randomise order of inputs\n",
    "            #np.random.shuffle(change)\n",
    "            #inputs = inputs[change,:]\n",
    "            #targets = targets[change,:]\n",
    "            \n",
    "        #return self.weights\n",
    "\n",
    "    def pcnfwd(self,inputs):\n",
    "        \"\"\" Run the network forward \"\"\"\n",
    "        # Compute activations\n",
    "        activations =  np.dot(inputs,self.weights)\n",
    "\n",
    "        # Threshold the activations\n",
    "        return np.where(activations>0,1,0)\n",
    "\n",
    "\n",
    "    def confmat(self,inputs,targets):\n",
    "        \"\"\"Confusion matrix\"\"\"\n",
    "\n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "        \n",
    "        outputs = np.dot(inputs,self.weights)\n",
    "    \n",
    "        nClasses = np.shape(targets)[1]\n",
    "\n",
    "        if nClasses==1:\n",
    "            nClasses = 2\n",
    "            outputs = np.where(outputs>0,1,0)\n",
    "        else:\n",
    "            # 1-of-N encoding\n",
    "            outputs = np.argmax(outputs,1)\n",
    "            targets = np.argmax(targets,1)\n",
    "\n",
    "        cm = np.zeros((nClasses,nClasses))\n",
    "        for i in range(nClasses):\n",
    "            for j in range(nClasses):\n",
    "                cm[i,j] = np.sum(np.where(outputs==i,1,0)*np.where(targets==j,1,0))\n",
    "\n",
    "        print(cm)\n",
    "        print(np.trace(cm)/np.sum(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's rewrite the code to run as sequential algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class pcn_sequential:\n",
    "    \"\"\" A basic Perceptron\"\"\"\n",
    "    \n",
    "    def __init__(self,inputs,targets):\n",
    "        \"\"\" Constructor \"\"\"\n",
    "        # Set up network size\n",
    "        if np.ndim(inputs)>1:\n",
    "            self.nIn = np.shape(inputs)[1]\n",
    "        else: \n",
    "            self.nIn = 1\n",
    "    \n",
    "        if np.ndim(targets)>1:\n",
    "            self.nOut = np.shape(targets)[1]\n",
    "        else:\n",
    "            self.nOut = 1\n",
    "\n",
    "        self.nData = np.shape(inputs)[0]\n",
    "    \n",
    "        # Initialise network\n",
    "        self.weights = np.random.rand(self.nIn+1,self.nOut)*0.1-0.05\n",
    "\n",
    "    def pcntrain(self,inputs,targets,eta,nIterations):\n",
    "        \"\"\" Train the thing \"\"\" \n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "        # Training\n",
    "        change = range(self.nData)\n",
    "        \n",
    "        nData = np.shape(inputs)[0]\n",
    "        M = np.shape(inputs)[1]\n",
    "        N = self.nOut\n",
    "\n",
    "        for n in range(nIterations):\n",
    "            \n",
    "            self.activations = self.pcnfwd(inputs)\n",
    "            \n",
    "            for data in range(nData):\n",
    "                for n in range(N):\n",
    "                    for m in range(M):\n",
    "                        self.weights[m, n] -= eta*inputs[data, m]*(self.activations[data, n] - targets[data, n])\n",
    "        \n",
    "            # Randomise order of inputs\n",
    "            #np.random.shuffle(change)\n",
    "            #inputs = inputs[change,:]\n",
    "            #targets = targets[change,:]\n",
    "            \n",
    "        #return self.weights\n",
    "\n",
    "    def pcnfwd(self,inputs):\n",
    "        \"\"\" Run the network forward \"\"\"\n",
    "        \n",
    "        nData = np.shape(inputs)[0]\n",
    "        M = np.shape(inputs)[1]\n",
    "        N = self.nOut\n",
    "        \n",
    "        activations = np.zeros((nData, N))\n",
    "        \n",
    "        for data in range(nData):\n",
    "            for n in range(N):\n",
    "\n",
    "                for m in range(M):\n",
    "                    activations[data, n] += self.weights[m, n] * inputs[data, m]\n",
    "\n",
    "                if activations[data, n] > 0:\n",
    "                    activations[data, n] = 1\n",
    "                else:\n",
    "                    activations[data, n] = 0\n",
    "\n",
    "        # Threshold the activations\n",
    "        return activations\n",
    "\n",
    "\n",
    "    def confmat(self,inputs,targets):\n",
    "        \"\"\"Confusion matrix\"\"\"\n",
    "\n",
    "        # Add the inputs that match the bias node\n",
    "        inputs = np.concatenate((inputs,-np.ones((self.nData,1))),axis=1)\n",
    "        \n",
    "        outputs = np.dot(inputs,self.weights)\n",
    "    \n",
    "        nClasses = np.shape(targets)[1]\n",
    "\n",
    "        if nClasses==1:\n",
    "            nClasses = 2\n",
    "            outputs = np.where(outputs>0,1,0)\n",
    "        else:\n",
    "            # 1-of-N encoding\n",
    "            outputs = np.argmax(outputs,1)\n",
    "            targets = np.argmax(targets,1)\n",
    "\n",
    "        cm = np.zeros((nClasses,nClasses))\n",
    "        for i in range(nClasses):\n",
    "            for j in range(nClasses):\n",
    "                cm[i,j] = np.sum(np.where(outputs==i,1,0)*np.where(targets==j,1,0))\n",
    "\n",
    "        print(cm)\n",
    "        print(np.trace(cm)/np.sum(cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inputData = np.matrix([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
    "labels = np.matrix([0, 1, 1, 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set parameters for each Perceptron type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eta = 0.25\n",
    "iterations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run batch version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.006588459014892578\n"
     ]
    }
   ],
   "source": [
    "pBatch = pcn_batch(inputData, labels.T)\n",
    "tBatchStart = time.time()\n",
    "pBatch.pcntrain(inputData, labels.T, eta, iterations)\n",
    "tBatchEnd = time.time()\n",
    "\n",
    "inputData = np.concatenate((inputData,-np.ones((np.shape(inputData)[0],1))),axis=1)\n",
    "pBatchPred = pBatch.pcnfwd(inputData)\n",
    "\n",
    "print(tBatchEnd - tBatchStart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run sequential version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.015163183212280273\n"
     ]
    }
   ],
   "source": [
    "pSeq = pcn_sequential(inputData, labels.T)\n",
    "tSeqStart = time.time()\n",
    "pSeq.pcntrain(inputData, labels.T, eta, iterations)\n",
    "tSeqEnd = time.time()\n",
    "\n",
    "inputData = np.concatenate((inputData,-np.ones((np.shape(inputData)[0],1))),axis=1)\n",
    "pSeqPred = pSeq.pcnfwd(inputData)\n",
    "\n",
    "print(tSeqEnd - tSeqStart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the batch version is much faster since the sequential version has three `for` loops. Looking at the outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch predictions:\n",
      "[[0]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n",
      "Sequential predictions:\n",
      "[[ 0.]\n",
      " [ 1.]\n",
      " [ 1.]\n",
      " [ 1.]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Batch predictions:\")\n",
    "print(pBatchPred)\n",
    "print(\"Sequential predictions:\")\n",
    "print(pSeqPred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both the batch and sequential versions obtain the same outputs. The batch version is ultimately a lot faster than the sequential one though."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
